# import json
# import os
# from datetime import datetime
# from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score

# # ==== CONFIG ====
# RESULT_PATH = "results/baseline_results_1199_20250710_XXXXXX.jsonl"  # ä½ å·²æœ‰çš„ç»“æœè·¯å¾„
# EVAL_SAVE = True  # æ˜¯å¦ä¿å­˜ eval_summary æ–‡ä»¶
# # ===============

# # ==== Load Results ====
# preds = []
# labels = []

# with open(RESULT_PATH, "r") as f:
#     for line in f:
#         ex = json.loads(line)
#         if "prediction" in ex and "label" in ex:
#             preds.append(ex["prediction"])
#             labels.append(ex["label"])

# if not preds:
#     raise ValueError("No predictions loaded!")

# # ==== Evaluation ====
# tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()
# accuracy = accuracy_score(labels, preds)
# pp = precision_score(labels, preds)
# pr = recall_score(labels, preds)
# npv = tn / (tn + fn) if (tn + fn) > 0 else 0
# nr = tn / (tn + fp) if (tn + fp) > 0 else 0

# # ==== Summary ====
# evaluation_summary = {
#     "result_path": RESULT_PATH,
#     "accuracy": round(accuracy, 4),
#     "precision": round(pp, 4),
#     "recall": round(pr, 4),
#     "negative_precision": round(npv, 4),
#     "negative_recall": round(nr, 4),
#     "true_negative": int(tn),
#     "false_positive": int(fp),
#     "false_negative": int(fn),
#     "true_positive": int(tp),
# }

# # ==== Save ====
# if EVAL_SAVE:
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     eval_path = f"results/eval_summary_from_results_{timestamp}.json"
#     with open(eval_path, "w") as f:
#         json.dump(evaluation_summary, f, indent=2)
#     print(f"\nâœ… Evaluation summary saved to: {eval_path}")

# # ==== Print ====
# print("\n===== Evaluation Result =====")
# for k, v in evaluation_summary.items():
#     print(f"{k}: {v}")




# è®¡ç®—é”™è¯¯ç±»å‹å¬å›ç‡å¹¶æ·»åŠ æ¯ä¸ª error_type çš„å¹³å‡ recall
from collections import defaultdict
import json
import os

# è·¯å¾„é…ç½®
SUBSET_PATH = "bug-data/NL2SQL-Bugs-Subset.json"
RESULT_PATH = "results/baseline_GEMINIresults_1199_20250711_031233.jsonl"
OUTPUT_PATH = "results/error_GEN.json"

# åŠ è½½åŸå§‹æ•°æ®å’Œé¢„æµ‹ç»“æœ
with open(SUBSET_PATH, "r") as f:
    raw_data = json.load(f)

with open(RESULT_PATH, "r") as f:
    results = [json.loads(line) for line in f]

stats = defaultdict(lambda: defaultdict(lambda: {"total": 0, "correct_detected": 0}))

# index raw_data by question
question_to_data = {d["question"]: d for d in raw_data}

matched = 0
skipped = 0

for r in results:
    q = r["question"]
    d = question_to_data.get(q)
    if d is None:
        skipped += 1
        continue

    matched += 1
    label = d["label"]
    pred = r["prediction"]
    errors = d.get("error_types", [])

    for err in errors:
        main_type = err["error_type"]
        sub_type = err["sub_error_type"]
        stats[main_type][sub_type]["total"] += 1
        if label is False and pred is False:
            stats[main_type][sub_type]["correct_detected"] += 1


# è®¡ç®— recall å¹¶æ ¼å¼åŒ–è¾“å‡º
result_summary = {}
for main_type, sub_dict in stats.items():
    result_summary[main_type] = {}
    recalls = []
    for sub_type, values in sub_dict.items():
        total = values["total"]
        correct = values["correct_detected"]
        recall = correct / total if total > 0 else 0
        recalls.append(recall)
        result_summary[main_type][sub_type] = {
            "total": total,
            "detected_false_positives": correct,
            "recall": round(recall, 4)
        }
    # æ·»åŠ å¹³å‡ recall
    if recalls:
        avg_recall = sum(recalls) / len(recalls)
        result_summary[main_type]["average_recall"] = round(avg_recall, 4)

# ä¿å­˜åˆ° JSON æ–‡ä»¶
with open(OUTPUT_PATH, "w") as f:
    json.dump(result_summary, f, indent=2)

print(f"âœ… Finished. Matched: {matched}, Skipped: {skipped}")
print("ğŸ“„ Saved to: {OUTPUT_PATH}")
